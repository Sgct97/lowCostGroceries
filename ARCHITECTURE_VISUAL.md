# ðŸ—ï¸ System Architecture - Visual Guide

## Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER (Frontend)                          â”‚
â”‚                                                                  â”‚
â”‚  1. User enters: ["milk", "eggs", "bread"] + ZIP 10001         â”‚
â”‚  2. Submit to API                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ POST /cart
                             â”‚ {"items": [...], "zipcode": "10001"}
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DROPLET 1: API + REDIS                         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              FastAPI (api.py)                          â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  1. Generate job_id: "abc-123"                        â”‚    â”‚
â”‚  â”‚  2. Create job_data with ZIP CODE                     â”‚    â”‚
â”‚  â”‚  3. Push to Redis queue                               â”‚    â”‚
â”‚  â”‚  4. Return job_id instantly                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                   â”‚                                             â”‚
â”‚                   â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Redis Server                              â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  Queue: scrape_queue                                  â”‚    â”‚
â”‚  â”‚    [job1, job2, job3, ...]                           â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  Status: status:abc-123                              â”‚    â”‚
â”‚  â”‚    {"status": "queued", "zip_code": "10001"}        â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  Results: result:abc-123                             â”‚    â”‚
â”‚  â”‚    (stored after completion)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Workers connect via Redis protocol
                             â”‚ (port 6379)
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DROPLET 2: WORKERS                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Worker 1 (worker.py)                                  â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚  Persistent UC Browser (warm)                â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Started once at boot                      â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Reused for multiple jobs                  â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Auto-restarts every 50 jobs               â”‚     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  Loop forever:                                        â”‚    â”‚
â”‚  â”‚    1. Pull job from Redis (blocking)                 â”‚    â”‚
â”‚  â”‚    2. Extract: items + ZIP CODE                      â”‚    â”‚
â”‚  â”‚    3. For each item:                                 â”‚    â”‚
â”‚  â”‚       - search("milk", zip_code="10001")           â”‚    â”‚
â”‚  â”‚       - search("eggs", zip_code="10001")           â”‚    â”‚
â”‚  â”‚       - search("bread", zip_code="10001")          â”‚    â”‚
â”‚  â”‚    4. Store results in Redis                         â”‚    â”‚
â”‚  â”‚    5. Repeat                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Worker 2 (worker.py)                                  â”‚    â”‚
â”‚  â”‚  [Same structure as Worker 1]                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Results stored in Redis
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER (Frontend)                          â”‚
â”‚                                                                  â”‚
â”‚  3. Poll: GET /results/abc-123 (every 2 seconds)               â”‚
â”‚                                                                  â”‚
â”‚  Response while processing:                                     â”‚
â”‚    {"status": "processing"}                                     â”‚
â”‚                                                                  â”‚
â”‚  Response when done:                                            â”‚
â”‚    {                                                            â”‚
â”‚      "status": "complete",                                      â”‚
â”‚      "results": {                                               â”‚
â”‚        "milk": [{price: 3.69, merchant: "Walmart"}, ...],     â”‚
â”‚        "eggs": [{price: 2.99, merchant: "Target"}, ...],      â”‚
â”‚        "bread": [{price: 2.49, merchant: "Kroger"}, ...]      â”‚
â”‚      },                                                         â”‚
â”‚      "zip_code": "10001"                                        â”‚
â”‚    }                                                            â”‚
â”‚                                                                  â”‚
â”‚  4. Display cheapest options to user                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Location-Specific Flow (CRITICAL)

```
User in NYC (ZIP 10001)
  â”‚
  â”œâ”€> API: job_data = {"zip_code": "10001", ...}
  â”‚
  â”œâ”€> Redis: Stores job with ZIP
  â”‚
  â”œâ”€> Worker pulls job
  â”‚   â”‚
  â”‚   â””â”€> Logs: ðŸ“ ZIP CODE: 10001 (LOCATION-SPECIFIC)
  â”‚   â”‚
  â”‚   â””â”€> search("milk", zip_code="10001")
  â”‚       â”‚
  â”‚       â””â”€> URL: "milk near zip 10001 nearby"
  â”‚           â”‚
  â”‚           â””â”€> Google returns: NYC stores
  â”‚
  â””â”€> Results: NYC products stored in Redis


User in Miami (ZIP 33101)  
  â”‚
  â”œâ”€> API: job_data = {"zip_code": "33101", ...}
  â”‚
  â”œâ”€> Redis: Stores job with ZIP
  â”‚
  â”œâ”€> Worker pulls job
  â”‚   â”‚
  â”‚   â””â”€> Logs: ðŸ“ ZIP CODE: 33101 (LOCATION-SPECIFIC)
  â”‚   â”‚
  â”‚   â””â”€> search("milk", zip_code="33101")
  â”‚       â”‚
  â”‚       â””â”€> URL: "milk near zip 33101 nearby"
  â”‚           â”‚
  â”‚           â””â”€> Google returns: Miami stores
  â”‚
  â””â”€> Results: Miami products stored in Redis
```

**Key Points:**
1. ZIP code is in the job data (from API)
2. Worker logs ZIP for verification
3. ZIP goes into search URL
4. Google returns location-specific results
5. **Same worker can handle different locations** because ZIP is in URL, not browser state

---

## Timing Breakdown (Per Cart with 10 Items)

```
Sequential Scraping (1 persistent browser):

Item 1 (milk):
  â”œâ”€ Navigate to URL: 1.0s
  â”œâ”€ Page load: 1.0s  
  â”œâ”€ Extract products: 0.5s
  â””â”€ Subtotal: 2.5s

Items 2-10 (eggs, bread, etc.):
  â”œâ”€ Navigate to URL: 0.5s (browser already warm)
  â”œâ”€ Page load: 0.8s
  â”œâ”€ Extract products: 0.3s
  â””â”€ Subtotal per item: 1.6s Ã— 9 = 14.4s

Browser overhead: 1.0s (initialization if cold start)

TOTAL: 2.5 + 14.4 + 1.0 = ~17-18 seconds
```

Compare to:
- **Original plan (fresh browser per cart):** ~37 seconds
- **Our optimization:** ~18 seconds
- **Savings:** 19 seconds (50% faster!)

---

## Scalability Model

```
2 Workers (Minimum Test):
  â”œâ”€ Concurrent capacity: 2 carts
  â”œâ”€ Time per cart: 18s
  â”œâ”€ Throughput: 2 Ã— (3600s / 18s) = 400 carts/hour
  â””â”€ Queue: Others wait in line

10 Workers:
  â”œâ”€ Concurrent capacity: 10 carts
  â”œâ”€ Time per cart: 18s
  â”œâ”€ Throughput: 10 Ã— 200 = 2,000 carts/hour
  â””â”€ Cost: ~5 droplets @ $24 = $120/month

50 Workers:
  â”œâ”€ Concurrent capacity: 50 carts
  â”œâ”€ Time per cart: 18s
  â”œâ”€ Throughput: 50 Ã— 200 = 10,000 carts/hour
  â””â”€ Cost: ~25 droplets @ $24 = $600/month

100 Workers:
  â”œâ”€ Concurrent capacity: 100 carts
  â”œâ”€ Time per cart: 18s
  â”œâ”€ Throughput: 100 Ã— 200 = 20,000 carts/hour
  â””â”€ Cost: ~50 droplets @ $24 = $1,200/month
```

**Linear scaling:** Double workers = Double capacity

---

## Error Handling & Recovery

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error Scenario: Browser Crashes                       â”‚
â”‚                                                         â”‚
â”‚  Worker detects crash                                  â”‚
â”‚    â”œâ”€> Log error                                       â”‚
â”‚    â”œâ”€> Store failure in Redis                         â”‚
â”‚    â”œâ”€> Start new browser                              â”‚
â”‚    â””â”€> Continue with next job                         â”‚
â”‚                                                         â”‚
â”‚  Job marked as "failed"                                â”‚
â”‚    â””â”€> User gets: {"status": "failed", "error": "..."} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error Scenario: CAPTCHA Detected                      â”‚
â”‚                                                         â”‚
â”‚  Worker detects CAPTCHA in HTML                        â”‚
â”‚    â”œâ”€> Log: "CAPTCHA detected"                        â”‚
â”‚    â”œâ”€> Restart browser (fresh session)                â”‚
â”‚    â””â”€> Job fails, but next job gets fresh browser     â”‚
â”‚                                                         â”‚
â”‚  Prevention:                                           â”‚
â”‚    â”œâ”€> Auto-restart every 50 jobs                     â”‚
â”‚    â”œâ”€> Auto-restart every 30 minutes                  â”‚
â”‚    â””â”€> Use real Chrome (not headless)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error Scenario: Redis Connection Lost                 â”‚
â”‚                                                         â”‚
â”‚  API detects Redis is down                             â”‚
â”‚    â”œâ”€> Log warning                                     â”‚
â”‚    â”œâ”€> Switch to DIRECT mode                          â”‚
â”‚    â””â”€> Scrape synchronously (slow but works)          â”‚
â”‚                                                         â”‚
â”‚  Worker detects Redis is down                          â”‚
â”‚    â”œâ”€> Log error                                       â”‚
â”‚    â”œâ”€> Wait 10 seconds                                 â”‚
â”‚    â””â”€> Retry connection                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Files Map

```
lowCostGroceries/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api.py                 â† API server (runs on Droplet 1)
â”‚   â”œâ”€â”€ worker.py              â† Worker script (runs on Droplet 2)
â”‚   â””â”€â”€ uc_scraper.py          â† UC scraper (imported by worker)
â”‚
â”œâ”€â”€ test_production_queue.py  â† End-to-end test
â”‚
â”œâ”€â”€ DEPLOYMENT_GUIDE.md        â† Step-by-step setup instructions
â”œâ”€â”€ PRODUCTION_READY_SUMMARY.md â† What was built & why
â””â”€â”€ ARCHITECTURE_VISUAL.md     â† This file (visual reference)
```

---

## Quick Reference Commands

### Start API (Droplet 1):
```bash
cd /root/app
export REDIS_HOST=localhost
python3 api.py
```

### Start Workers (Droplet 2):
```bash
cd /root
export REDIS_HOST=YOUR_DROPLET_1_IP
xvfb-run -a python3 worker.py > worker1.log 2>&1 &
xvfb-run -a python3 worker.py > worker2.log 2>&1 &
```

### Monitor Queue:
```bash
redis-cli -h YOUR_DROPLET_1_IP llen scrape_queue
```

### Monitor Workers:
```bash
tail -f /root/worker1.log | grep "ZIP CODE"
```

### Run Test:
```bash
python3 test_production_queue.py
```

---

## Summary

This architecture gives you:

âœ… **Fast** - 18s per cart (50% faster than before)  
âœ… **Scalable** - Add workers = more capacity  
âœ… **Reliable** - Queue handles any load  
âœ… **Location-accurate** - ZIP code preserved and verified  
âœ… **Cost-effective** - $36/month to test, $600/month for production  

**Ready to deploy!** ðŸš€

